# Loading necessary libraries.
library(data.table)
library(tidyverse)
library(gridExtra)
# Loading the dataset.
data <- fread("exile_meta.tsv", header = TRUE, sep = "\t", data.table = FALSE)
# Extracting information about the newspaper Välis-Eesti.
valiseesti_combined <- data %>%
filter(str_detect(docid, "valiseesti")) %>%
filter(year >= 1944 & year <= 1991) %>%
group_by(year) %>%
summarize(count = n()) %>%
mutate(object_name = "Välis-Eesti")
# Calculating the sum of all pages.
valiseesti_sum <- sum(valiseesti_combined$count)
valiseesti_sum #8361
# Graph for Välis-Eesti.
valiseesti <- ggplot(valiseesti_combined, aes(x = factor(year), y = count)) +
geom_bar(stat = "identity", fill = "#00BFC4") +
labs(title = "Count of pages per year: Välis-Eesti",
x = "Year",
y = "Count of pages") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Extracting information about Eesti Päevaleht and Stockholms-Tidningen Eestlastele.
result_estdagbladet <- data %>%
filter(str_detect(docid, "estdagbladet")) %>%
filter(year >= 1944 & year <= 1991) %>%
group_by(year) %>%
summarize(count = n()) %>%
mutate(object_name = "Eesti Päevaleht")
result_stockholmstid <- data %>%
filter(str_detect(docid, "stockholmstid")) %>%
filter(year >= 1944 & year <= 1991) %>%
group_by(year) %>%
summarize(count = n()) %>%
mutate(object_name = "Stockholms-Tidningen\nEestlastele")
# Combining the results.
estdagbladet_stockholmstid_combined <- bind_rows(result_estdagbladet, result_stockholmstid) %>%
group_by(year, object_name) %>%
summarise(count = sum(count))
# Calculating the sum of all pages.
estdagbladet_sum <- sum(result_estdagbladet$count)
estdagbladet_sum #25773
stockholmstid_sum <- sum(result_stockholmstid$count)
stockholmstid_sum #5529
# Combining the results.
estdagbladet_stockholmstid_sum <- sum(estdagbladet_stockholmstid_combined$count)
estdagbladet_stockholmstid_sum #31302
# Graph for Eesti Päevaleht and Stockholms-Tidningen Eestlastele.
estdagbladet_stockholmstid <- ggplot(estdagbladet_stockholmstid_combined, aes(x = factor(year), y = count, fill = object_name)) +
geom_bar(stat = "identity") +
labs(title = "Count of pages per year: Eesti Päevaleht / Stockholms-Tidningen Eestlastele",
x = "Year",
y = "Count of pages",
fill = "Newspaper") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Extracting information about Teataja and Eesti Teataja.
result_eestiteatajastock <- data %>%
filter(str_detect(docid, "eestiteatajastock")) %>%
filter(year >= 1944 & year <= 1991) %>%
group_by(year) %>%
summarize(count = n()) %>%
mutate(object_name = "Eesti Teataja")
result_teatajapoliit <- data %>%
filter(str_detect(docid, "teatajapoliit")) %>%
filter(year >= 1944 & year <= 1991) %>%
group_by(year) %>%
summarize(count = n()) %>%
mutate(object_name = "Teataja")
# Combining the results.
eestiteatajastock_teatajapoliit_combined <- bind_rows(result_eestiteatajastock, result_teatajapoliit) %>%
group_by(year, object_name) %>%
summarise(count = sum(count))
# All pages summarised.
eestiteatajastock_sum <- sum(result_eestiteatajastock$count)
eestiteatajastock_sum #3339
teatajapoliit_sum <- sum(result_teatajapoliit$count)
teatajapoliit_sum #8430
# Combining the results.
eestiteatajastock_teatajapoliit_sum <- sum(eestiteatajastock_teatajapoliit_combined$count)
eestiteatajastock_teatajapoliit_sum #11769
# Graph for Teataja and Eesti Teataja.
eestiteatajastock_teatajapoliit <- ggplot(eestiteatajastock_teatajapoliit_combined, aes(x = factor(year), y = count, fill = object_name)) +
geom_bar(stat = "identity") +
labs(title = "Count of pages per year: Teataja / Eesti Teataja",
x = "Year",
y = "Count of pages",
fill = "Newspaper") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Combining the plots using grid.arrange. The combined plot shows the newspapers' page counts per year.
combined_plot <- grid.arrange(valiseesti, estdagbladet_stockholmstid, eestiteatajastock_teatajapoliit, ncol = 1)
# Saving the combined plot.
ggsave("newspapers_by_pages.png", plot = combined_plot, width = 10, height = 6, units = "in")
# Combining all the results.
all_newspapers <- bind_rows(valiseesti_combined, estdagbladet_stockholmstid_combined, eestiteatajastock_teatajapoliit_combined)
all_newspapers$object_name <- gsub("Eesti Päevaleht|Stockholms-Tidningen\nEestlastele", "Eesti Päevaleht / Stockholms-Tidningen Eestlastele", all_newspapers$object_name)
all_newspapers$object_name <- gsub("Eesti Teataja|Teataja", "Teataja / Eesti Teataja", all_newspapers$object_name)
all_newspapers <- aggregate(count ~ year + object_name, data = all_newspapers, sum)
# Calculating the overall average page count per year for all newspapers.
overall_avg <- all_newspapers %>%
group_by(year) %>%
summarize(overall_avg_count = mean(count))
overall_mean <- mean(overall_avg$overall_avg_count)
overall_mean
# Creating the graph, which shows the combined average page counts per year as a barplot and different newspapers' page counts on line plots.
ggplot() +
geom_bar(data = overall_avg, aes(x = year, y = overall_avg_count), stat = "identity", fill = "gray40") +
geom_line(data = all_newspapers, aes(x = year, y = count, group = object_name, color = object_name), linewidth=1.2) +
labs(title = "Changes in the number of newspaper pages per year with overall average",
x = "Year",
y = "Pages",
color = "Newspaper")
# Saving the combined plot.
ggsave("newspapers_w_avg.png", width = 10, height = 6, units = "in")
# Combining all the results.
all_newspapers <- bind_rows(valiseesti_combined, estdagbladet_stockholmstid_combined, eestiteatajastock_teatajapoliit_combined)
all_newspapers$object_name <- gsub("Eesti Päevaleht|Stockholms-Tidningen\nEestlastele", "Eesti Päevaleht\n/ Stockholms-Tidningen Eestlastele", all_newspapers$object_name)
all_newspapers$object_name <- gsub("Eesti Teataja|Teataja", "Teataja / Eesti Teataja", all_newspapers$object_name)
all_newspapers <- aggregate(count ~ year + object_name, data = all_newspapers, sum)
# Calculating the overall average page count per year for all newspapers.
overall_avg <- all_newspapers %>%
group_by(year) %>%
summarize(overall_avg_count = mean(count))
overall_mean <- mean(overall_avg$overall_avg_count)
overall_mean
# Creating the graph, which shows the combined average page counts per year as a barplot and different newspapers' page counts on line plots.
ggplot() +
geom_bar(data = overall_avg, aes(x = year, y = overall_avg_count), stat = "identity", fill = "gray40") +
geom_line(data = all_newspapers, aes(x = year, y = count, group = object_name, color = object_name), linewidth=1.2) +
labs(title = "Changes in the number of newspaper pages per year with overall average",
x = "Year",
y = "Pages",
color = "Newspaper")
# Saving the combined plot.
ggsave("newspapers_w_avg.png", width = 10, height = 6, units = "in")
# Combining all the results.
all_newspapers <- bind_rows(valiseesti_combined, estdagbladet_stockholmstid_combined, eestiteatajastock_teatajapoliit_combined)
all_newspapers$object_name <- gsub("Eesti Päevaleht|Stockholms-Tidningen\nEestlastele", "Eesti Päevaleht\n/ Stockholms-Tidningen\nEestlastele", all_newspapers$object_name)
all_newspapers$object_name <- gsub("Eesti Teataja|Teataja", "Teataja / Eesti Teataja", all_newspapers$object_name)
all_newspapers <- aggregate(count ~ year + object_name, data = all_newspapers, sum)
# Calculating the overall average page count per year for all newspapers.
overall_avg <- all_newspapers %>%
group_by(year) %>%
summarize(overall_avg_count = mean(count))
overall_mean <- mean(overall_avg$overall_avg_count)
overall_mean
# Creating the graph, which shows the combined average page counts per year as a barplot and different newspapers' page counts on line plots.
ggplot() +
geom_bar(data = overall_avg, aes(x = year, y = overall_avg_count), stat = "identity", fill = "gray40") +
geom_line(data = all_newspapers, aes(x = year, y = count, group = object_name, color = object_name), linewidth=1.2) +
labs(title = "Changes in the number of newspaper pages per year with overall average",
x = "Year",
y = "Pages",
color = "Newspaper")
# Saving the combined plot.
ggsave("newspapers_w_avg.png", width = 10, height = 6, units = "in")
# Loading necessary packages.
library(quanteda)
library(quanteda.textstats)
library(tidyverse)
# Read the "kodu" concordances file into R.
file_path <- "kodu_concs.txt"
concordances <- readLines(file_path)
setwd("C:/GitHub/LNU_MA/text_analysis")
# Read the "kodu" concordances file into R.
file_path <- "kodu_concs.txt"
concordances <- readLines(file_path)
stopwords_file <- "estonian-stopwords-lemmas.txt"
stopwords <- readLines(stopwords_file)
# Creating a corpus from the concordances.
corpus <- corpus(concordances)
# Finding collocates.
tokens <- tokens(corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE)
collocations <- textstat_collocations(tokens, min_count = 10)
collocations_filtered <- collocations %>%
filter(grepl("kodu", collocation)) %>%
arrange(desc(lambda))
collocations_filtered
# Creating a DFM.
tokens2 <- tokens_remove(tokens, stopwords)
dfm.sentences <- dfm(tokens2)
similarity.words <- textstat_simil(dfm.sentences, dfm.sentences[,"kodu"], margin = "features", method = "cosine")
head(similarity.words[order(similarity.words[,1], decreasing = T),], 11)
# Loading necessary packages.
library(tidyverse)
setwd("C:/GitHub/LNU_MA/spatial_analysis")
# Entering output path.
output_directory <- "C:/GitHub/LNU_MA/spatial_analysis"
# Get a list of files that follow the pattern. Although the code is meant for looping a folder of files, it also suits for processing an individual file due to distinct pattern.
file_list <- list.files(pattern = "_\\d+_locations\\.txt")
# Looping over the files.
for (file in file_list) {
# Reading the files.
valiseesti <- readLines(file, encoding = "UTF-8")
# Harmonising the results.
valiseesti <- tolower(valiseesti)
valiseesti <- gsub("usa-[[:alpha:]]+", "usa", valiseesti) # Fixing the grammatical cases behind USA.
valiseesti <- gsub("[^[:alpha:]0-9 -]", "", valiseesti) # Erasing everything except the non-alphabetical and numerical characters, whitespaces and hyphens.
valiseesti <- gsub("p p| p | p$", "", valiseesti) # Erasing previously unremoved html paragraph marks.
valiseesti <- gsub("\\s+", " ", valiseesti) # Replacing multiple whitespaces with single whitespace.
valiseesti <- gsub(" $", "", valiseesti) # Removing whitespaces at the end of the locations.
valiseesti <- gsub("^\\s+", "", valiseesti) # Removing whitespaces at the beginning of the locations.
# Converting the modified content back to a data frame.
valiseesti <- data.frame(V1 = valiseesti, stringsAsFactors = FALSE)
# Counting occurrences of each place name.
identsed <- valiseesti %>%
group_by(V1) %>%
summarize(count = n())
identsed <- identsed[order(identsed$count,decreasing = TRUE),]
# Writing unique place names to a file in the output directory.
write.table(identsed, file.path(output_directory, paste0("unique_", file)), sep="\t", quote = FALSE, row.names = FALSE)
# Getting the top 1% most frequent place names.
top_1_percent <- identsed %>%
arrange(desc(count)) %>%
slice_head(n = floor(0.01 * nrow(.)))
# Writing the top 1% most frequent place names to a file in the output directory.
write.table(top_1_percent, file.path(output_directory, paste0("top_1_percent_", file)), sep="\t", quote = FALSE, row.names = FALSE)
}
# Getting list of files starting with "top_1_percent_".
top_files <- list.files(path = output_directory, pattern = "^top_1_percent_", full.names = TRUE)
# Initialising an empty data frame to store merged data.
merged_data <- data.frame()
# Looping over each file and read its contents into merged_data.
for (file in top_files) {
# Reading the file.
top_data <- read.table(file, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
# Merging the data.
merged_data <- rbind(merged_data, top_data)
}
# Grouping and summarising the locations.
merged_summarized <- merged_data %>%
group_by(V1) %>%
summarize(count = sum(count)) %>%
arrange(desc(count)) # Sort by count in descending order
# Writing the merged and summarized data to a file.
write.table(merged_summarized, file.path(output_directory, "merged_top_1_percent_sorted.txt"), sep="\t", quote = FALSE, row.names = FALSE)
# Loading necessary libraries.
library(tidyverse)
library(scales)
# Reading the data from the text file.
data <- read.table("unique_august_91_locations.txt", sep = "\t", header = TRUE)
# Taking only the top 20 most frequent locations.
data <- head(data, 20)
# Creating the bar plot using ggplot2.
ggplot(data, aes(x = count, y = reorder(V1, count))) +
geom_bar(stat = "identity", fill = "#00BFC4") +
labs(title = "20 most frequent locations: August 1991", x = "Frequency", y = "Location") +
scale_x_continuous(labels = comma) +  # Format x-axis labels as integers
theme_minimal() +
theme(axis.text.y = element_text(hjust = 0))
ggsave("20_most_frequent_locations.png", width = 10, height = 6, units = "in")
# Creating the bar plot using ggplot2.
ggplot(data, aes(x = count, y = reorder(V1, count))) +
geom_bar(stat = "identity", fill = "#00BFC4") +
labs(title = "20 most frequent locations: August 1991", x = "Frequency", y = "Location") +
scale_x_continuous(labels = comma) +  # Format x-axis labels as integers
theme(axis.text.y = element_text(hjust = 0))
ggsave("20_most_frequent_locations.png", width = 10, height = 6, units = "in")
ggsave("20_most_frequent_locations.png", width = 600, height = 300, units = "px")
ggsave("20_most_frequent_locations.png", scale = 1, width = 600, height = 300, units = "px")
ggsave("20_most_frequent_locations.png", width = 990, height = 495, units = "px")
# Loading necessary libraries.
library(dplyr)
# Reading the unique locations by decade.
file1 <- read.table("unique_august_91_locations.txt", sep="\t", header=TRUE, stringsAsFactors=FALSE)
# Reading the merged coordinates file.
file2 <- read.table("merged_coordinates.txt", sep=",", header=TRUE, stringsAsFactors=FALSE)
# Performing a left join to keep the order of file1.
merged_df <- left_join(file1, file2[, c("V1", "lat", "lon")], by="V1")
# Replacing NA values with "NO MATCH" only where there's no corresponding V1 value in file2.
merged_df <- merged_df %>%
mutate(lat = ifelse(is.na(lat) & V1 %in% setdiff(file1$V1, file2$V1), "NO MATCH", lat),
lon = ifelse(is.na(lon) & V1 %in% setdiff(file1$V1, file2$V1), "NO MATCH", lon))
merged_df <- merged_df %>%
filter(!grepl("eesti", V1))
View(merged_df)
# Performing a left join to keep the order of file1.
merged_df <- left_join(file1, file2[, c("V1", "lat", "lon")], by="V1")
# Replacing NA values with "NO MATCH" only where there's no corresponding V1 value in file2.
merged_df <- merged_df %>%
mutate(lat = ifelse(is.na(lat) & V1 %in% setdiff(file1$V1, file2$V1), "NO MATCH", lat),
lon = ifelse(is.na(lon) & V1 %in% setdiff(file1$V1, file2$V1), "NO MATCH", lon))
# Saving the merged DataFrame to a new CSV file.
write.csv(merged_df, "august_91_coordinates.csv", row.names=FALSE, quote = FALSE)
# Saving the merged DataFrame to a new CSV file.
write.csv(merged_df, "august_91_coordinates.txt", row.names=FALSE, quote = FALSE)
# Reading the CSV file containing coordinates data.
coordinates_data <- read.csv("august_91_coordinates.txt")
View(coordinates_data)
# Defining broadly the most northern, southern, eastern and western coordinates of Estonia.
estonia_bbox <- list(
north = 59.68,
south = 57.51,
east = 28.21,
west = 21.83
)
# Filtering coordinates within the borders.
coordinates_in_estonia <- subset(coordinates_data,
lat >= estonia_bbox$south &
lat <= estonia_bbox$north &
lon >= estonia_bbox$west &
lon <= estonia_bbox$east)
# Deleting all the place names containing the word "eesti", as these hide other locations.
coordinates_in_estonia <- coordinates_in_estonia %>%
filter(!grepl(specific_word, V1))
# Deleting all the place names containing the word "eesti", as these hide other locations.
coordinates_in_estonia <- coordinates_in_estonia %>%
filter(!grepl("eesti", V1))
# Writing the filtered data to a new CSV file.
write.csv(coordinates_in_estonia, "august_91_estonia_coordinates.txt", row.names = FALSE)
# Writing the filtered data to a new CSV file.
write.csv(coordinates_in_estonia, "august_91_estonia_coordinates.txt", row.names = FALSE, quote = FALSE)
setwd("C:/GitHub/LNU_MA/spatial_analysis/data")
